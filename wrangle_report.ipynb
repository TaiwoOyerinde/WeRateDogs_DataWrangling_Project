{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wrangle_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction:\n",
    "In this project, my task was to wrangle, analyze and visualize data from three different sources on the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Data:\n",
    "1. In gathering the first dataset, I manually downloaded a file,*twitter-archive-enhanced.csv*, from Udacity's project workspace and loaded it as a dataframe`tweets` on my local jupyter notebook using pandas `-read_cv`.\n",
    "2. For the second dataset gathering process, a link that contains [tweet image predictions]( 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv') was provided in the Udacity's Classroom. I used the **Requests** library to programmatically download the tweets,*image_predictions.tsv*, locally on my desktop and loaded it as a dataframe `image_tweets`. \n",
    "3. The last dataset used the Twitter API to gather data. I got my twitter developer account approved, retrieved all my access tokens and secrets, and ran codes for about 30 minutes using the 'tweet_id'in `tweets`( I used the guide given in the Udacity workspace,*twitter-api.py*) to gather additional data which downloaded on my desktop as *tweet_json.txt* . I initialized an empty list *tweets_data* , loaded the relevant data I needed(ids, retweets and favorite counts) and appended it to my empty list. Then, I converted the list *tweets_data* to a dataframe using `pd.DataFrame` and named it `tweets_extra`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Data:\n",
    "I visually assessed the dataset using python and my text editor, Notepad.Then,I programmatically assessed the 3 datasets to detect data quality and tidiness issues. I documented a number of data quality issues and 2 data tidiness issues as a result of my assessment in the `wrangle_act.ipynb`. The issues are seen below: \n",
    "#### Quality issues\n",
    "\n",
    "1. tweet_id should be a string to avoid statistical calculations on them.\n",
    "2. timestamp should be datetime. \n",
    "3. some tweets have several identical expanded_urls in `tweets`.\n",
    "4. we only need 'original' tweets; retweets and replies are not such. We have to drop the following columns-'in_reply_to_status_id', 'in_reply_to_user_id', 'retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp' . \n",
    "5. source column is clustered. We have to extract the relevant source names from them. \n",
    "6. name column has invalid name values in them. We need to replace them with None. \n",
    "7. Inconsistency in Null Values. \n",
    "8. 'rating_numerator' and 'rating_denominator' not properly defined.\n",
    "9. some tweets have identical jpg_urls. \n",
    "10. In the `image_tweets` table, the different predictions of dog breeds,confidence level and ascertainty of dog(p1,p1_conf,p1_dog,p2,p2_conf,...) breaches tidy data format- Each variable forms a column. We have to combine these columns to give us only 2 columns; suggested dog breed and confidence level. \n",
    "\n",
    "#### Tidiness issues\n",
    "\n",
    "1. In the `tweets`table, the dog stages in the 4 columns instead of 1. DONE\n",
    "\n",
    "2. The 3 datasets- `tweets`, `image_tweets` and `tweets_extra`should be merged as 1. DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data:\n",
    "Before cleaning my data, I made copies of all 3 datasets. To clean my data, I took each of the issues documented from my assessing activity as stated above and ran them through the Define-Code-Test process. I carefully defined the problem, ran code(s) to solve the problem and then tested them to confirm the issue is resolved. \n",
    "> At the end of the cleaning process, I successfully combined all 3 datasets into 1,`twitter`, which I stored as *twitter-archive-master.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
